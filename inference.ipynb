{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eae0015-a5c6-493e-832a-0cfcb0ef128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the model config\n",
    "from params import *\n",
    "\n",
    "# importing minLlama3\n",
    "from model import *\n",
    "\n",
    "# used in the training loop\n",
    "import time\n",
    "\n",
    "# used to save & load models\n",
    "import json\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0918a651-5c6a-4a39-8b3e-a28259e4fd64",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c0ba50-83de-4ad7-b262-944e6d547ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1968.256 K parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Llama3(\n",
       "  (tok_embeddings): Embedding(256, 128)\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (wk): Linear(in_features=128, out_features=32, bias=False)\n",
       "        (wv): Linear(in_features=128, out_features=32, bias=False)\n",
       "        (wo): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=128, out_features=512, bias=False)\n",
       "        (w2): Linear(in_features=512, out_features=128, bias=False)\n",
       "        (w3): Linear(in_features=128, out_features=512, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=128, out_features=256, bias=False)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretrained model options:\n",
    "#    2m parameters, context length = 256, trained for 500 iterations w/ batch size of 32 and no dropout: 'Llama3_2024-04-19|04-00-15'\n",
    "name = 'Llama3_2024-04-19|04-00-15'\n",
    "\n",
    "# Deserialize the JSON file back to a dictionary\n",
    "with open(f'models/{name}.json', 'r') as f:\n",
    "    params_dict = json.load(f)\n",
    "\n",
    "# Convert the dictionary back to a dataclass object\n",
    "params = ModelArgs(**params_dict)\n",
    "params.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize a blank model\n",
    "model = Llama3(params, tokenizer).to(params.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = f'models/{name}.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path)) \n",
    "# REMEMBER TO CHANGE VALUES IN params TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c971fce-8b3e-4732-bd66-d5d2028025d6",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16fa4207-7db1-4c22-8e4c-481389f73ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\" # the classic line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbfcf3d2-b62d-458c-9fbd-bb90969f5193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo sleep\n",
      "the first be the dish of the sea, where you are too,\n",
      "The king of my friends, and the city of the world.\n",
      "\n",
      "Second Citizen:\n",
      "He is the ground to signifier who to give\n",
      "the princes of the pitchance of the victory.\n",
      "\n",
      "MERCUTIO:\n",
      "You was the man the world of William Sir William Saint Plantagenet?\n",
      "\n",
      "LUCIO:\n",
      "He comes your way will prove you what he weep\n"
     ]
    }
   ],
   "source": [
    "# doing everything with default values\n",
    "print(model.generate(input_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf903ce-2edf-44cf-b808-b39da658d7f0",
   "metadata": {},
   "source": [
    "#### now let's use memory_saver_div to take advantage of KV caching for linear scaling of memory usage with sequence length increase in exchange for *potential* quality degradation. memory_saver_div must be a power of 2, and it is used to calculate the maximum length of the query's sequence length dimension in the attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93b8d42e-d926-4477-8ce1-26e3bcb1779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum attention matrix size will be 64x256 rather than 256x256\n",
      "\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Rome,\n",
      "When I am a look that shall we were a gods,\n",
      "I cannot recompany to the king to please the blood.\n",
      "\n",
      "First Servingman:\n",
      "O, good some little blood with her brother and plain as whose founds,\n",
      "Which is this day of his extremity:\n",
      "I have a princely spirit of the field,\n",
      "And when a man of his common grow.\n",
      "\n",
      "POMPEY:\n",
      "Well, be \n"
     ]
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    input_str, \n",
    "    max_gen_len = params.max_seq_len - len(input_str), # our model doesn't have a built-in <endoftext> token so we have to specify when to stop generating\n",
    "    memory_saver_div = 4, # the largest value we'll allow our query sequence length to get. makes memory consumption linear with respect to sequence length\n",
    "    temperature = 0.6, # this is the default value that Llama3's official code has set\n",
    "    top_p = 0.9, # this is the default value that Llama3's official code has set\n",
    "    top_k = 16, # meta's code doesn't actually implement top_k selection but i've added it anyways as an alternative\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af2d78a-1d5b-42eb-85ad-0e486deb314a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
