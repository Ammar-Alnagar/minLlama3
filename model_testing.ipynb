{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3509d342-f233-40d7-91d1-d4e865bd60ab",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "\n",
    "- [ ] figure out how to implement memory_saver_div into the kv cache\n",
    "- [ ] add dropout\n",
    "- [ ] train bigger version (longer context length?)\n",
    "- [ ] copy & paste into model.py\n",
    "- [ ] make a train.py\n",
    "- [ ] make a params.py\n",
    "- [ ] build colab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb200416-d927-4302-b6b2-a0bfe159c321",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ae964-36e4-4a3a-8109-306ce7cac387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562f334c-b16b-42ab-830c-03761b4daf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# imports for the tokenizer\n",
    "from tiny_shakespeare_tokenizer import *\n",
    "\n",
    "# Imports used for the params\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "# for the dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "import math\n",
    "\n",
    "# used in the training loop\n",
    "import time\n",
    "\n",
    "# used to save & load models\n",
    "import json\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236635c-f158-4aa5-80e1-3ed28f0de035",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd373bf-d326-4939-be18-819bae892805",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(size = 256) # size options are 128, 256, 512 and 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ef54bc-7b2c-49f7-ae86-86d9c05b9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 128 # 4096\n",
    "    n_layers: int = 8 # 32\n",
    "    n_heads: int = 4 # 32\n",
    "    n_kv_heads: Optional[int] = 1 # None\n",
    "    vocab_size: int = tokenizer.vocab_len # -1\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "    rope_theta: float = 10000 # 500000\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 256 # 2048\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "params = ModelArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d246596-a8f5-445c-b759-8f59c3205ed5",
   "metadata": {},
   "source": [
    "# RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937ec822-e4fb-4b76-9ada-37dd1f94acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce58c9d-f195-474a-8dfd-1259c92288f7",
   "metadata": {},
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96691d68-93d1-4f72-8ae2-cb1c3db1b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2432e6b1-0334-4bb2-9528-b1ce729d3ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1]), f'freqs_cis.shape {freqs_cis.shape} != (x.shape[1], x.shape[-1]) {(x.shape[1], x.shape[-1])}'\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca5e1b5-6f8f-4944-b4a5-845bac6b3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604f297-9e41-4b94-9f3d-69bc71a0318e",
   "metadata": {},
   "source": [
    "# ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8571ff0e-6c22-4d64-91c1-2131719a4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    bs, seqlen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, seqlen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, seqlen, n_kv_heads * n_rep, head_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "181d512a-8fa4-4e54-a511-19e07a9fd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        self.n_rep = args.n_heads // self.n_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim),\n",
    "            requires_grad = False\n",
    "        ).to(args.device)\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim),\n",
    "            requires_grad = False\n",
    "        ).to(args.device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "        start_pos: int = None,\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        if start_pos is not None: # if we're performing inference, use kv caching\n",
    "            self.cache_k = self.cache_k.to(xq)\n",
    "            self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "            #print(start_pos, seqlen, self.cache_k.shape, xk.shape)\n",
    "            self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "            self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "    \n",
    "            keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "            values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "        else: # if we're training, do full sequence length\n",
    "            keys, values = xk, xv\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        keys = repeat_kv(keys, self.n_rep)  # (bs, cache_len + seqlen, n_heads, head_dim)\n",
    "        values = repeat_kv(values, self.n_rep)  # (bs, cache_len + seqlen, n_heads, head_dim)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (bs, n_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2)  # (bs, n_heads, cache_len + seqlen, head_dim)\n",
    "        values = values.transpose(1, 2)  # (bs, n_heads, cache_len + seqlen, head_dim)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_heads, seqlen, head_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b53056-2b01-4294-8536-87f064febf7f",
   "metadata": {},
   "source": [
    "# FFWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3111446c-52ff-4c36-b441-e6724d6ad461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb6e48-7b65-489e-82d7-3432a3b6d75a",
   "metadata": {},
   "source": [
    "# Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd1fcf1-a1dd-4e6c-8d78-e267398304f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "        start_pos: int = None,\n",
    "    ):\n",
    "        h = x + self.attention(self.attention_norm(x), freqs_cis, mask, start_pos)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bca78-a613-4d1f-b930-76d4f66fa266",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c0c7cab-2436-4c54-b9ec-f77c6e5456f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3(nn.Module):\n",
    "    def __init__(self, params: ModelArgs, tokenizer):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "        self.max_seq_len = params.max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(\n",
    "            params.dim, \n",
    "            params.vocab_size, \n",
    "            bias=False)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            params.dim // params.n_heads,\n",
    "            params.max_seq_len * 2,\n",
    "            params.rope_theta,)\n",
    "\n",
    "        mask = torch.full((params.max_seq_len, params.max_seq_len), \n",
    "                          float(\"-inf\"), \n",
    "                          device=params.device)\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, # specifically for training\n",
    "                tokens: torch.Tensor, \n",
    "                targets: torch.Tensor):\n",
    "        bsz, seqlen = tokens.shape\n",
    "        assert tokens.shape == targets.shape\n",
    "        assert seqlen == self.max_seq_len\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[:seqlen]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            h = layer(h, freqs_cis, self.mask, start_pos=None)\n",
    "        h = self.norm(h)\n",
    "        logits = self.output(h).float()\n",
    "\n",
    "        loss = self.criterion(\n",
    "            logits.view(bsz * seqlen, self.vocab_size),\n",
    "            targets.reshape(bsz * seqlen))\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward_inference(self, \n",
    "                          tokens: torch.Tensor,\n",
    "                          start_pos: int,\n",
    "                          max_context_window: int,\n",
    "                         ):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = self.mask[:seqlen, :seqlen]\n",
    "        # When performing key-value caching, we compute the attention scores\n",
    "        # only for the new sequence. Thus, the matrix of scores is of size\n",
    "        # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n",
    "        # j > cache_len + i, since row i corresponds to token cache_len + i.\n",
    "        mask = torch.hstack(\n",
    "            [torch.zeros((seqlen, start_pos), device=tokens.device), mask]\n",
    "        ).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, freqs_cis, mask, start_pos=start_pos)\n",
    "        h = self.norm(h)\n",
    "        logits = self.output(h).float()\n",
    "        return logits\n",
    "\n",
    "    @torch.inference_mode() # no need to keep track of gradients during inference\n",
    "    def Sampler(\n",
    "        self,\n",
    "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
    "        temperature: float, # controls how boring vs random the outputs should be\n",
    "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
    "        top_k: int, # the maximum number of output options we're willing to consider\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The Sampler function is responsible for generating token predictions\n",
    "        It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling \n",
    "        \"\"\"\n",
    "        # Select the last element for each sequence.\n",
    "        logits = logits[:,-1,:] # (batch_size, input_len, vocab_size) -> (batch_size, vocab_size)\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        logits.div_(temperature) # (batch_size, vocab_size) / float -> (batch_size, vocab_size)\n",
    "\n",
    "        # Calculate probabilities with softmax.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along\n",
    "\n",
    "        # sort the probabilities to for use in top-p & top-k. both are (batch_size, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "\n",
    "        ### calculating top-p\n",
    "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) \n",
    "        # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
    "        # the original probabilities with excluded tokens changed to 0.0\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort) \n",
    "\n",
    "        ### calculating top_k\n",
    "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) \n",
    "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
    "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "        top_ks_mask = top_ks_mask >= top_k\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        # this trims probs_sort to also fit within our top_k requirement\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "\n",
    "        # Re-normalization so that total probabilities add up to 1\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        \n",
    "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort, dim=-1, index=torch.argsort(probs_idx, dim=-1))\n",
    "        \n",
    "        # samples from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        return next_token_id # returns the predicted token\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_gen_len: int = None,\n",
    "        memory_saver_div: int = 1, # defaults to full max_seq_len**2 memory use. must be power of 2\n",
    "        temperature: float = 0.6, # default value in meta's code\n",
    "        top_p: float = 0.9, # default value in meta's code\n",
    "        top_k: int = 32, # meta's code doesn't bother with topk\n",
    "    ) -> str: \n",
    "        \"\"\" Wrapper around sampler() that deals with manipulation of the sequence \"\"\"\n",
    "        assert ((memory_saver_div & (memory_saver_div-1)) == 0) & (memory_saver_div > 0), f'memory_saver_div {memory_saver_div} must be power of 2'\n",
    "        max_context_window = self.max_seq_len // memory_saver_div\n",
    "        \n",
    "        # encoding the prompt into token indices\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "        \n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.max_seq_len - len(tokens)\n",
    "        elif max_gen_len + len(tokens) > self.max_seq_len:\n",
    "            print(f'capping max_gen_len at max_seq_len={self.max_seq_len} including input')\n",
    "            max_gen_len = self.max_seq_len - len(tokens)\n",
    "\n",
    "        # turning it into the right tensor shape\n",
    "        tokens = torch.tensor(tokens, device=self.params.device)\n",
    "        tokens = tokens.unsqueeze(0) if len(tokens.shape)==1 else tokens # jic we need to add a batch dimension\n",
    "        \n",
    "        start_pos = max(tokens.shape[1] - max_context_window, 0)\n",
    "        \n",
    "        for i in range(max_gen_len):\n",
    "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "            logits = self.forward_inference(\n",
    "                tokens[:,:self.max_seq_len],\n",
    "                start_pos = start_pos,\n",
    "                max_context_window = max_context_window\n",
    "            )\n",
    "            \n",
    "            next_token = self.Sampler(\n",
    "                logits = logits,\n",
    "                temperature = temperature,\n",
    "                top_p = top_p,\n",
    "                top_k = top_k\n",
    "            )\n",
    "\n",
    "            # add our new token to the sequence\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "            \n",
    "            if tokens.shape[1] >= self.max_seq_len:\n",
    "                start_pos += 1\n",
    "\n",
    "        # decode our list of tokens to an actual string\n",
    "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a1f17-5a2e-4bbe-bc99-e9412d2b81de",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c131b4ce-c393-4885-bd37-9a4651c7fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a21f4739-9e5c-480c-9223-80bcc81c3330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9dc2aa7-6a7a-4723-90b9-d00f9ea03b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - params.max_seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+params.max_seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+params.max_seq_len+1] for i in ix])\n",
    "    x, y = x.to(params.device), y.to(params.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9475574f-acff-43a7-ae1b-97ca30f4d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1968.256 K parameters\n",
      "Llama3(\n",
      "  (tok_embeddings): Embedding(256, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (wk): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (wv): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (wo): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (w2): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (w3): Linear(in_features=128, out_features=512, bias=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=128, out_features=256, bias=False)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Llama3(params, tokenizer).to(params.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba904bf4-cfbd-4a30-aafe-d76971283b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 5): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, targets=Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3fd4135-760e-4f67-be11-24bdb497e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "lr_init = 1e-2\n",
    "weight_decay = 0.02\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr_init, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 500\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 25\n",
    "\n",
    "# Warmup setup\n",
    "warmup_iters = 25  # Number of warmup iterations\n",
    "warmup_factor = 1e-2  # Warmup factor (initial learning rate is multiplied by this factor)\n",
    "\n",
    "lr_final = 1e-5  # Minimum learning rate\n",
    "\n",
    "def lr_lambda(current_iter):\n",
    "    if current_iter < warmup_iters:\n",
    "        # Warmup phase\n",
    "        return warmup_factor + (1 - warmup_factor) * current_iter / warmup_iters\n",
    "    else:\n",
    "        # Cosine decay phase with minimum learning rate\n",
    "        decay_iters = max_iters - warmup_iters\n",
    "        cosine_decay = 0.5 * (1 + math.cos(math.pi * (current_iter - warmup_iters) / decay_iters))\n",
    "        return max(cosine_decay, lr_final / lr_init)\n",
    "        \n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: lr 0.000496, train loss 5.5877, val loss 5.5893, time elapsed: 1.07 seconds\n",
      "step 25: lr 0.010000, train loss 3.2807, val loss 3.4082, time elapsed: 25.13 seconds\n",
      "step 50: lr 0.009926, train loss 3.1071, val loss 3.2393, time elapsed: 49.10 seconds\n",
      "step 75: lr 0.009718, train loss 2.9478, val loss 3.1434, time elapsed: 73.62 seconds\n",
      "step 100: lr 0.009382, train loss 2.8066, val loss 3.0346, time elapsed: 97.60 seconds\n",
      "step 125: lr 0.008925, train loss 2.7220, val loss 3.0126, time elapsed: 122.02 seconds\n",
      "step 150: lr 0.008362, train loss 2.6286, val loss 2.9169, time elapsed: 146.24 seconds\n",
      "step 175: lr 0.007707, train loss 2.5054, val loss 2.8463, time elapsed: 170.00 seconds\n",
      "step 200: lr 0.006978, train loss 2.4186, val loss 2.7004, time elapsed: 194.29 seconds\n",
      "step 225: lr 0.006195, train loss 2.3221, val loss 2.6358, time elapsed: 218.16 seconds\n",
      "step 250: lr 0.005380, train loss 2.2511, val loss 2.5394, time elapsed: 242.06 seconds\n",
      "step 275: lr 0.004554, train loss 2.1958, val loss 2.5247, time elapsed: 266.81 seconds\n",
      "step 300: lr 0.003741, train loss 2.1486, val loss 2.5025, time elapsed: 290.72 seconds\n",
      "step 325: lr 0.002961, train loss 2.1024, val loss 2.4423, time elapsed: 315.33 seconds\n",
      "step 350: lr 0.002238, train loss 2.0969, val loss 2.4177, time elapsed: 339.37 seconds\n",
      "step 375: lr 0.001589, train loss 2.0198, val loss 2.3657, time elapsed: 363.83 seconds\n",
      "step 400: lr 0.001034, train loss 1.9922, val loss 2.4023, time elapsed: 387.89 seconds\n",
      "step 425: lr 0.000587, train loss 1.9698, val loss 2.3747, time elapsed: 411.71 seconds\n",
      "step 450: lr 0.000260, train loss 1.9897, val loss 2.3300, time elapsed: 435.48 seconds\n",
      "step 475: lr 0.000063, train loss 1.9802, val loss 2.3548, time elapsed: 459.24 seconds\n",
      "step 499: lr 0.000010, train loss 1.9349, val loss 2.3451, time elapsed: 482.37 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', params.max_batch_size)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, targets=yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, params.max_batch_size)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"step {iter:04d}: lr {current_lr:.6f}, train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'models/{model.__class__.__name__}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}'\n",
    "torch.save(model.state_dict(), f'{name}.pth')\n",
    "\n",
    "# Convert the dataclass object to a dictionary\n",
    "params_dict = asdict(params)\n",
    "\n",
    "# Serialize the dictionary to a JSON file\n",
    "with open(f'{name}.json', 'w') as f:\n",
    "    json.dump(params_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5242a7-6035-4f53-9d26-0690aea809d9",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16d5b32a-8fc1-4fa2-a301-85142c442bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1968.256 K parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Llama3(\n",
       "  (tok_embeddings): Embedding(256, 128)\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (wk): Linear(in_features=128, out_features=32, bias=False)\n",
       "        (wv): Linear(in_features=128, out_features=32, bias=False)\n",
       "        (wo): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=128, out_features=512, bias=False)\n",
       "        (w2): Linear(in_features=512, out_features=128, bias=False)\n",
       "        (w3): Linear(in_features=128, out_features=512, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=128, out_features=256, bias=False)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'Llama3_2024-04-19|04-00-15'\n",
    "\n",
    "# Deserialize the JSON file back to a dictionary\n",
    "with open(f'models/{name}.json', 'r') as f:\n",
    "    params_dict = json.load(f)\n",
    "\n",
    "# Convert the dictionary back to a dataclass object\n",
    "params = ModelArgs(**params_dict)\n",
    "\n",
    "# Initialize a blank model\n",
    "model = Llama3(params, tokenizer).to(params.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = f'models/{name}.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path)) \n",
    "# REMEMBER TO CHANGE VALUES IN params TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30e834-56f9-4812-ab4c-03e2f14874ca",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2260c4f-40b2-4b8e-9db4-ad1717a3027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou shalt a bond,\n",
      "And marry to the blood of work with me,\n",
      "So shall be more to a true work to the king.\n",
      "\n",
      "KING RICHARD III:\n",
      "We will not speak a bastard did in the rest,\n",
      "As I am in bright of the triumphant,\n",
      "And you have great a dead the house of it,\n",
      "That prays of him in the first a while,\n",
      "Or back in the hour of the world whom you\n",
      "W\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou \" # the classic line\n",
    "max_useable_output_len = params.max_seq_len - len(input_str)\n",
    "output = model.generate(input_str, max_gen_len = max_useable_output_len)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f258187-bf1d-482c-9c96-9cf21be21daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Richard of Henry to you.\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "Why, sir, sir.\n",
      "\n",
      "MENENIUS:\n",
      "What is my lordship.\n",
      "\n",
      "KING RICHARD III:\n",
      "So what I will do see the prince, and would a word,\n",
      "Who have his work of this slave from the light,\n",
      "And to be protest of the state of death\n",
      "That would I know the rest spirit, I would have\n",
      "That thus may speak the city of \n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\" # the classic line\n",
    "max_useable_output_len = params.max_seq_len - len(input_str)\n",
    "output = model.generate(input_str, max_gen_len = max_useable_output_len, memory_saver_div = 4)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7ef80-36ef-4153-a1f2-de17fa396ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
