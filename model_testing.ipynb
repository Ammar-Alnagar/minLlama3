{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3509d342-f233-40d7-91d1-d4e865bd60ab",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "\n",
    "- [ ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb200416-d927-4302-b6b2-a0bfe159c321",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ae964-36e4-4a3a-8109-306ce7cac387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562f334c-b16b-42ab-830c-03761b4daf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# imports for the tokenizer\n",
    "from tiny_shakespeare_tokenizer import *\n",
    "\n",
    "# Imports used for the config\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "# for the dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "\n",
    "# used in the training loop\n",
    "import time\n",
    "\n",
    "# used to save & load models\n",
    "import json\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236635c-f158-4aa5-80e1-3ed28f0de035",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd373bf-d326-4939-be18-819bae892805",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(size = 256) # size options are 128, 256, 512 and 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ef54bc-7b2c-49f7-ae86-86d9c05b9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 128 # 4096\n",
    "    n_layers: int = 4 # 32\n",
    "    n_heads: int = 4 # 32\n",
    "    n_kv_heads: Optional[int] = 1 # None\n",
    "    vocab_size: int = tokenizer.vocab_len # -1\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "    rope_theta: float = 10000 # 500000\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 128 # 2048\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "config = ModelArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d246596-a8f5-445c-b759-8f59c3205ed5",
   "metadata": {},
   "source": [
    "# RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "937ec822-e4fb-4b76-9ada-37dd1f94acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce58c9d-f195-474a-8dfd-1259c92288f7",
   "metadata": {},
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96691d68-93d1-4f72-8ae2-cb1c3db1b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2432e6b1-0334-4bb2-9528-b1ce729d3ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ca5e1b5-6f8f-4944-b4a5-845bac6b3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604f297-9e41-4b94-9f3d-69bc71a0318e",
   "metadata": {},
   "source": [
    "# ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "181d512a-8fa4-4e54-a511-19e07a9fd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        self.n_rep = args.n_heads // self.n_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim),\n",
    "            requires_grad = False\n",
    "        ).to(args.device)\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim),\n",
    "            requires_grad = False\n",
    "        ).to(args.device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "        start_pos: int = None,\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        if start_pos is not None: # if we're performing inference, use kv caching\n",
    "            self.cache_k = self.cache_k.to(xq)\n",
    "            self.cache_v = self.cache_v.to(xq)\n",
    "    \n",
    "            self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "            self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "    \n",
    "            keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "            values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "        else: # if we're training, do full sequence length\n",
    "            keys, values = xk, xv\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        keys = repeat_kv(keys, self.n_rep)  # (bs, cache_len + seqlen, n_heads, head_dim)\n",
    "        values = repeat_kv(values, self.n_rep)  # (bs, cache_len + seqlen, n_heads, head_dim)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (bs, n_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2)  # (bs, n_heads, cache_len + seqlen, head_dim)\n",
    "        values = values.transpose(1, 2)  # (bs, n_heads, cache_len + seqlen, head_dim)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_heads, seqlen, head_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b53056-2b01-4294-8536-87f064febf7f",
   "metadata": {},
   "source": [
    "# FFWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3111446c-52ff-4c36-b441-e6724d6ad461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb6e48-7b65-489e-82d7-3432a3b6d75a",
   "metadata": {},
   "source": [
    "# Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbd1fcf1-a1dd-4e6c-8d78-e267398304f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "        start_pos: int = None,\n",
    "    ):\n",
    "        h = x + self.attention(self.attention_norm(x), freqs_cis, mask, start_pos)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c0c7cab-2436-4c54-b9ec-f77c6e5456f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3(nn.Module):\n",
    "    def __init__(self, params: ModelArgs, tokenizer):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "        self.max_seq_len = params.max_seq_len\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(\n",
    "            params.dim, \n",
    "            params.vocab_size, \n",
    "            bias=False)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            params.dim // params.n_heads,\n",
    "            params.max_seq_len * 2,\n",
    "            params.rope_theta,)\n",
    "\n",
    "        mask = torch.full((params.max_seq_len, params.max_seq_len), \n",
    "                          float(\"-inf\"), \n",
    "                          device=params.device)\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, \n",
    "                tokens: torch.Tensor, \n",
    "                start_pos: int = None, \n",
    "                targets: torch.Tensor = None):\n",
    "        if start_pos is not None:\n",
    "            return self.forward_inference(tokens, start_pos)\n",
    "        elif targets is not None:\n",
    "            return self.forward_training(tokens, targets)\n",
    "        else:\n",
    "            raise InputError('must be performing either training or inference. both start_pos and targets cannot be NoneType')\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward_inference(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = self.mask[:seqlen, :seqlen]\n",
    "        # When performing key-value caching, we compute the attention scores\n",
    "        # only for the new sequence. Thus, the matrix of scores is of size\n",
    "        # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n",
    "        # j > cache_len + i, since row i corresponds to token cache_len + i.\n",
    "        mask = torch.hstack(\n",
    "            [torch.zeros((seqlen, start_pos), device=tokens.device), mask]\n",
    "        ).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output, None # no loss\n",
    "\n",
    "    def forward_training(self, tokens: torch.Tensor, targets: torch.Tensor):\n",
    "        bsz, seqlen = tokens.shape\n",
    "        assert tokens.shape == targets.shape\n",
    "        assert seqlen == self.max_seq_len\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        freqs_cis = self.freqs_cis.to(h.device)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            h = layer(h, freqs_cis, self.mask, start_pos=None)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "\n",
    "        loss = self.criterion(\n",
    "            logits.view(bsz * seqlen, self.vocab_size),\n",
    "            target_token_ids.reshape(bsz * seqlen))\n",
    "        \n",
    "        return output, loss\n",
    "\n",
    "    @torch.inference_mode() # no need to keep track of gradients during inference\n",
    "    def Sampler(\n",
    "        self,\n",
    "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
    "        temperature: float, # controls how boring vs random the outputs should be\n",
    "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
    "        top_k: int, # the maximum number of output options we're willing to consider\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The Sampler function is responsible for generating token predictions\n",
    "        It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling \n",
    "        \"\"\"\n",
    "        # Select the last element for each sequence.\n",
    "        logits = logits[:,-1,:] # (batch_size, input_len, vocab_size) -> (batch_size, vocab_size)\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        logits.div_(temperature) # (batch_size, vocab_size) / float -> (batch_size, vocab_size)\n",
    "\n",
    "        # Calculate probabilities with softmax.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along\n",
    "\n",
    "        # sort the probabilities to for use in top-p & top-k. both are (batch_size, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "\n",
    "        ### calculating top-p\n",
    "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) \n",
    "        # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
    "        # the original probabilities with excluded tokens changed to 0.0\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort) \n",
    "\n",
    "        ### calculating top_k\n",
    "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) \n",
    "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
    "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "        top_ks_mask = top_ks_mask >= top_k\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        # this trims probs_sort to also fit within our top_k requirement\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "\n",
    "        # Re-normalization so that total probabilities add up to 1\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        \n",
    "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort, dim=-1, index=torch.argsort(probs_idx, dim=-1))\n",
    "        \n",
    "        # samples from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        return next_token_id # returns the predicted token\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output_len: int = None, \n",
    "        temperature: float = 0.6, # default value in meta's code\n",
    "        top_p: float = 0.9, # default value in meta's code\n",
    "        top_k: int = 32, # meta's code doesn't bother with topk\n",
    "    ) -> str: \n",
    "        \"\"\" Wrapper around sampler() that deals with manipulation of the sequence \"\"\"\n",
    "        # encoding the prompt into token indices\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "        \n",
    "        if output_len is None:\n",
    "            output_len = self.max_seq_len - len(tokens)\n",
    "\n",
    "        # turning it into the right tensor shape\n",
    "        tokens = torch.tensor(tokens, device=self.params.device).unsqueeze(0)\n",
    "        \n",
    "        # we wouldn't want to go past the maximum context length we trained on\n",
    "        #assert len(tokens) + output_len <= self.max_seq_len\n",
    "        # actually with my current kv cache setup we kinda gots to\n",
    "\n",
    "        for i in range(output_len):\n",
    "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "            logits, _ = self(tokens[:,:self.max_seq_len], \n",
    "                             start_pos = len(tokens) - self.max_seq_len)\n",
    "            \n",
    "            next_token = self.Sampler(\n",
    "                logits = logits,\n",
    "                temperature = temperature,\n",
    "                top_p = top_p,\n",
    "                top_k = top_k\n",
    "            )\n",
    "\n",
    "            # add our new token to the sequence\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "        # decode our list of tokens to an actual string\n",
    "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a1f17-5a2e-4bbe-bc99-e9412d2b81de",
   "metadata": {},
   "source": [
    "# Training-related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c131b4ce-c393-4885-bd37-9a4651c7fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a21f4739-9e5c-480c-9223-80bcc81c3330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9dc2aa7-6a7a-4723-90b9-d00f9ea03b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.max_seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.max_seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.max_seq_len+1] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba904bf4-cfbd-4a30-aafe-d76971283b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters = 5): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = process_batch(next(val_loader_iter), tokenizer, config)\n",
    "            logits, loss = model(X.to(config.device), Y.to(config.device))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9475574f-acff-43a7-ae1b-97ca30f4d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016.96 K parameters\n",
      "Llama3(\n",
      "  (tok_embeddings): Embedding(256, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (wk): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (wv): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (wo): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (w2): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (w3): Linear(in_features=128, out_features=512, bias=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=128, out_features=256, bias=False)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Llama3(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3fd4135-760e-4f67-be11-24bdb497e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 2\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m.\u001b[39mmax_batch_size)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(xb, targets\u001b[38;5;241m=\u001b[39myb)\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[58], line 41\u001b[0m, in \u001b[0;36mLlama3.forward\u001b[0;34m(self, tokens, start_pos, targets)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_inference(tokens, start_pos)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_training(tokens, targets)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InputError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmust be performing either training or inference. both start_pos and targets cannot be NoneType\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[58], line 75\u001b[0m, in \u001b[0;36mLlama3.forward_training\u001b[0;34m(self, tokens, targets)\u001b[0m\n\u001b[1;32m     72\u001b[0m freqs_cis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreqs_cis\u001b[38;5;241m.\u001b[39mto(h\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 75\u001b[0m     h \u001b[38;5;241m=\u001b[39m layer(h, freqs_cis, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask, start_pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     76\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(h)\n\u001b[1;32m     77\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(h)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, freqs_cis, mask, start_pos)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     20\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     start_pos: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m ):\n\u001b[0;32m---> 25\u001b[0m     h \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_norm(x), freqs_cis, mask, start_pos)\n\u001b[1;32m     26\u001b[0m     out \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_norm(h))\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[62], line 37\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, freqs_cis, mask, start_pos)\u001b[0m\n\u001b[1;32m     34\u001b[0m xk \u001b[38;5;241m=\u001b[39m xk\u001b[38;5;241m.\u001b[39mview(bsz, seqlen, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m     35\u001b[0m xv \u001b[38;5;241m=\u001b[39m xv\u001b[38;5;241m.\u001b[39mview(bsz, seqlen, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m---> 37\u001b[0m xq, xk \u001b[38;5;241m=\u001b[39m apply_rotary_emb(xq, xk, freqs_cis\u001b[38;5;241m=\u001b[39mfreqs_cis)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;66;03m# if we're performing inference, use kv caching\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_k\u001b[38;5;241m.\u001b[39mto(xq)\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(xq, xk, freqs_cis)\u001b[0m\n\u001b[1;32m      6\u001b[0m xq_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_complex(xq\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mxq\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m      7\u001b[0m xk_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_complex(xk\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mxk\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m freqs_cis \u001b[38;5;241m=\u001b[39m reshape_for_broadcast(freqs_cis, xq_)\n\u001b[1;32m      9\u001b[0m xq_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(xq_ \u001b[38;5;241m*\u001b[39m freqs_cis)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     10\u001b[0m xk_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(xk_ \u001b[38;5;241m*\u001b[39m freqs_cis)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36mreshape_for_broadcast\u001b[0;34m(freqs_cis, x)\u001b[0m\n\u001b[1;32m      2\u001b[0m ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mndim\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m ndim\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m freqs_cis\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      5\u001b[0m shape \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m freqs_cis\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mshape)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', config.max_batch_size)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, targets=yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'models/{model.__class__.__name__}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}'\n",
    "torch.save(model.state_dict(), f'{name}.pth')\n",
    "\n",
    "# Convert the dataclass object to a dictionary\n",
    "config_dict = asdict(config)\n",
    "\n",
    "# Serialize the dictionary to a JSON file\n",
    "with open(f'{name}.json', 'w') as f:\n",
    "    json.dump(config_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5242a7-6035-4f53-9d26-0690aea809d9",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16d5b32a-8fc1-4fa2-a301-85142c442bd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/?.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/?.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the saved state dictionary\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print the number of parameters in the model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK parameters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/?.pth'"
     ]
    }
   ],
   "source": [
    "name = '?'\n",
    "\n",
    "# Deserialize the JSON file back to a dictionary\n",
    "with open(f'models/{name}.json', 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "# Convert the dictionary back to a dataclass object\n",
    "config = Config(**config_dict)\n",
    "\n",
    "# Initialize a blank model\n",
    "model = Model(config, tokenizer).to(config.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = f'models/{name}.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path)) \n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30e834-56f9-4812-ab4c-03e2f14874ca",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2260c4f-40b2-4b8e-9db4-ad1717a3027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou ouplayitx-. ennIigSlemWQarppribebePam8ttinayGozouonOUhTheyutGlo G;KWomroufandhaiheTheywststayall:Hplay?ppooz;;beanKKTheyeramsaDdari\n",
      "uO43\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou \" # the classic line\n",
    "max_useable_output_len = config.max_seq_len - len(input_str)\n",
    "output = model.generate(input_str, output_len = max_useable_output_len, temperature=50.0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f258187-bf1d-482c-9c96-9cf21be21daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c0a4474-2503-452b-8054-e5d129557c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode('JULIET:\\nO Romeo, Romeo! wherefore art thou 9b\\nerAk!ot11EEerr$L$$$toplayplayplay2dwSou1waswas. sdha9ll,wasulTheJheomJJ, zplayplayTheyplayver?zEer;NP5000baThey?playplayplaver, stonmm4allCiPPha2CenenhiswawasirXandiMor,A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7ef80-36ef-4153-a1f2-de17fa396ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
